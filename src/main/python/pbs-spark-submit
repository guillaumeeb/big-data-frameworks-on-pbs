#!/usr/bin/python -u
#
# pbs-spark-submit:  Run an Apache Spark "job" (including optionally
#                    starting the Spark services) inside a PBS job.
# Copyright 2014, 2015 University of Tennessee
#
# License:  GNU GPL v2; see ../COPYING for details.
# Revision info:
# $HeadURL$
# $Revision$
# $Date$
import getopt
import glob
import os
import platform
import sys
import time

#
# ways to launch workers
#
class Launcher:
    def launch(self,cmdline,env,propagate_env=False,
               prop_env_list=["SPARK_CONF_DIR","SPARK_LOG_DIR","SPARK_LOCAL_DIRS"]):
        raise NotImplementedError
    def sleep(self):
        sleeptime = 5
        if ( "PBS_NUM_NODES" in os.environ.keys() ):
            sleeptime += 2*int(os.environ["PBS_NUM_NODES"])
        time.sleep(sleeptime)
    def env_list(env,prop_env_list):
        # since we can't rely on ssh_config and sshd_config having
        # the appropriate SendEnv/AcceptEnv settings
        argv = []
        for var in prop_env_list:
            if ( var in env.keys() ):
                argv.append(var+"="+env[var])
        return argv
    def env_string(env,prop_env_list):
        return " ".join(self.env_list(env,prop_env_list))

#
# Launcher implementation using pbsdsh. Not working on linux3 cluster
#
class PBSDSHLauncher(Launcher):
    def launch(self,cmdline,env,propagate_env=False,
               prop_env_list=["SPARK_CONF_DIR","SPARK_LOG_DIR","SPARK_LOCAL_DIRS"]):
        time.sleep(1)
        cmd = cmdline
        if ( propagate_env ):
            cmd = self.env_string(env,prop_env_list)+" "+cmdline
        #pbsdsh is not functionning on PBS cluster... Don't know why.
        print(cmd)
        os.system("pbsdsh -v -- " + cmd + " &\n")
        self.sleep()

#
# Ssh launcher. It works, making it the default.
#
class SSHLauncher(Launcher):
    def launch(self,cmdline,env,propagate_env=False,
               prop_env_list=["SPARK_CONF_DIR","SPARK_LOG_DIR","SPARK_LOCAL_DIRS"]):
        time.sleep(1)
        if ( "PBS_NODEFILE" in os.environ.keys() ):
            # Grep all nodes from this pbs submit for starting workers.
            nodefile = open(os.environ["PBS_NODEFILE"])
            for line in nodefile.readlines():
                argv = cmdline.split()
                node = line.rstrip("\n")
                ssh = "ssh"
                if ( "SPARK_SSH" in env.keys() ):
                    ssh=env["SPARK_SSH"]
                argv.insert(0,ssh)
                argv.insert(1,node)
                if ( propagate_env ):
                    for arg in self.env_list(env,prop_env_list):
                        argv.insert(2,arg)
                sys.stderr.write(" ".join(argv)+"\n")
                child_pid = os.fork()
                if ( child_pid==0 ):
                    os.execvpe(argv[0],argv,env)   
            nodefile.close()
            self.sleep()#Why a sleep is performed?
        else:
            raise EnvironmentError("PBS_NODEFILE undefined")


#
# functions to help with handling Java properties
#
def propsToCmdLine(proplist):
    result = []
    for prop in proplist.keys():
        result.append("-D"+prop+"=\""+proplist[prop]+"\"")
    return " ".join(result)

def propsFromFile(filename):
    if ( not os.path.exists(filename) ):
        raise IOError(filename+" not found")
    proplist = {}
    fd = open(filename)
    for line in fd.readlines():
        if ( not line.startswith("#") ):
            keyval = (line.rstrip("\n")).split("=",1)
            if ( len(keyval)==2 ):
                proplist[keyval[0]] = keyval[1]
    return proplist

#
# documentation
# TODO: To many options: some of thema re available through env variables too.
#
def usage():
    sys.stderr.write("pbs-spark-submit:  Run an Apache Spark \"job\" (including optionally\n\tstarting the Spark master and work services) inside a PBS job.\n")
    sys.stderr.write("\n")
    sys.stderr.write("Usage:  pbs-spark-submit [arguments] <app.jar|python_file> [app options]\n")
    sys.stderr.write("\n")
    sys.stderr.write("Options:\n")
    sys.stderr.write("\t--help or -h\n\t\tPrint this help message\n")
    sys.stderr.write("\t--init\n\t\tInitialize Spark master/worker services (default).\n")
    sys.stderr.write("\t--no-init\n\t\tDo not initialize Spark master/worker services.\n")
    sys.stderr.write("\t--pbsdsh\n\t\tUse the pbsdsh process launcher.\n")
    sys.stderr.write("\t--ssh\n\t\tUse the ssh process launcher (default).\n")
    sys.stderr.write("\t--conf-dir <confdir> or -C <confdir>\n\t\tLook in <confdir> for Java properties files.\n")
    sys.stderr.write("\t--log-dir <logdir> or -L <logdir>\n\t\tPlace logs in <logdir>.\n")
    sys.stderr.write("\t--log4j-properties <propsfile> or -l <propsfile>\n\t\tRead log4j properties from <propsfile>.\n")
    sys.stderr.write("\t--work-dir <workdir> or -d <workdir>\n\t\tUse <workdir> as Spark program's working directory.\n")
    sys.stderr.write("\t--memory <memlimit> or -m <memlimit>\n\t\tSet per-worker memory limit.\n")
    sys.stderr.write("\t--cores <cpulimit> or -c <cpulimit>\n\t\tSet per-worker CPU limit.\n")
    sys.stderr.write("\t--pausetime <N> or -p <N>\n\t\tPause <N> seconds between startup stages (default 5).\n")
    sys.stderr.write("\t-D <key>=<value>\n\t\tSet the Java property <key> to <value>.\n")
    sys.stderr.write("\t--properties-file <propfile> or -P <propfile>\n\t\tRead Java properties from <propfile>.\n")
    sys.stderr.write("\n")
    sys.stderr.write("Run \"man pbs-spark-submit\" for more details.\n")
    sys.stderr.write("\n")
    sys.exit(0)
    

#
# main program begins here
#

# sanity checks
if ( not ( "PBS_JOBID" in os.environ.keys() ) ):
    raise EnvironmentError("Not in a PBS job")
if ( not ( "SPARK_HOME" in os.environ.keys() ) ):
    raise EnvironmentError("SPARK_HOME not defined")

# set up default environment
init_svcs = True
memlimit = None
cpulimit = None
child_args = []
properties = {}
pausetime = 3
launcher = PBSDSHLauncher()
log4j_props = None

if ( "SPARK_LAUNCHER" in os.environ.keys() ):
    if ( os.environ["SPARK_LAUNCHER"] in ("pbsdsh","PBSDSH") ):
        launcher = PBSDSHLauncher()
    if ( os.environ["SPARK_LAUNCHER"] in ("ssh","SSH") ):
        launcher = SSHLauncher()
if ( not ( "SPARK_CONF_DIR" in os.environ.keys() ) ):
    os.environ["SPARK_CONF_DIR"] = os.getcwd()+"/conf"
if ( not ( "SPARK_LOG_DIR" in os.environ.keys() ) ):
    os.environ["SPARK_LOG_DIR"] = os.getcwd()

# manage scratch directories
# **ASSUMPTION**:  work directory is on a shared file system
workdir = os.getcwd()
if ( "SCRATCHDIR" in os.environ.keys() ):
    workdir = os.environ["SCRATCHDIR"]+"/spark-"+os.environ["PBS_JOBID"]
# SPARK_LOCAL_DIRS should be node-local
if ( ( "TMPDIR" in os.environ.keys() ) and
     not ( "SPARK_LOCAL_DIRS" in os.environ.keys() ) ):
    os.environ["SPARK_LOCAL_DIRS"] = os.environ["TMPDIR"]
elif ( not ( "SPARK_LOCAL_DIRS" in os.environ.keys() ) ):
    os.environ["SPARK_LOCAL_DIRS"] = "/tmp"

# command line argument handling
try:
    opts, child_args = getopt.getopt(sys.argv[1:],"C:D:d:hL:l:m:P:p:c:",["help","init","no-init","pbsdsh","ssh","conf-dir","log-dir=","log4j-properties=","work-dir=","memory=","cores=","properties-file","pause-time"])
except getopt.GetoptError, err:
    sys.stderr.write(str(err)+"\n")
    usage()
for opt in opts:
    if ( opt[0]=="--no-init" ):
        init_svcs = False
    elif ( opt[0]=="--init" ):
        init_svcs = True
    elif ( opt[0]=="--help" or opt[0]=="-h" ):
        usage()
    elif ( opt[0]=="--pbsdsh" ):
        launcher = PBSDSHLauncher()
    elif ( opt[0]=="--ssh" ):
        launcher = SSHLauncher()
    elif ( opt[0]=="--conf-dir" or opt[0]=="-C" ):
        os.environ["SPARK_CONF_DIR"] = opt[1]
    elif ( opt[0]=="--log-dir" or opt[0]=="-L" ):
        os.environ["SPARK_LOG_DIR"] = opt[1]
    elif ( opt[0]=="--log4j-properties" or opt[0]=="-l" ):
        log4j_props = opt[1]
    elif ( opt[0]=="--work-dir" or opt[0]=="-d" ):
        workdir = opt[1]
    elif ( opt[0]=="--memory" or opt[0]=="-m" ):
        memlimit = opt[1]
    elif ( opt[0]=="--cores" or opt[0]=="-c" ):
        cpulimit = opt[1]
    elif ( opt[0]=="-D" ):
        keyval = opt[1].split("=",1)
        if ( len(keyval)==2 ):
            properties[keyval[0]] = keyval[1]
        else:
            raise getopt.GetoptError("malformed property \""+opt[1]+"\"")
    elif ( opt[0]=="--properties-file" or opt[0]=="-P" ):
        if ( os.path.exists(opt[1]) ):
            props = propsFromFile(opt[1])
            for key in props.keys():
                properties[key] = props[key]
    elif ( opt[0]=="--pause-time" or opt[0]=="-p" ):
        pausetime = opt[1]

# read any properties files in the conf directory
for propfile in glob.glob(os.environ["SPARK_CONF_DIR"]+"/*.properties"):
    if ( os.path.exists(propfile) ):
        props = propsFromFile(propfile)
        for key in props.keys():
            if ( not ( key in properties.keys() ) ):
                properties[key] = props[key]

# make sure the work dir actually exists
if ( workdir is not None and not os.path.exists(workdir) ):
    os.mkdir(workdir)


# **ASSUMPTION**:  master runs on mother superior node, this could change in the future?
os.environ["SPARK_MASTER_IP"] = platform.node()
if ( not ( "SPARK_MASTER_PORT" in os.environ.keys() ) ):
    os.environ["SPARK_MASTER_PORT"] = "7077"
#Dont put spark_master in environment, this causes problem with Spark trying to rsync it, which is not needed here.
spark_master = "spark://"+os.environ["SPARK_MASTER_IP"]+":"+str(os.environ["SPARK_MASTER_PORT"])
#sys.stderr.write("Spark master = "+os.environ["SPARK_MASTER"]+"\n")

if ( init_svcs ):
    # stick any properties in the appropriate environment variable
    if ( len(properties)>0 ):
        if ( "SPARK_DAEMON_JAVA_OPTS" in os.environ.keys() ):
            os.environ["SPARK_DAEMON_JAVA_OPTS"] += " "+propsToCmdLine(properties)
        else:
            os.environ["SPARK_DAEMON_JAVA_OPTS"] = propsToCmdLine(properties)

    # launch master on mother superior
    cmdline = os.environ["SPARK_HOME"]+"/sbin/start-master.sh"
    print("Launching master " + spark_master)
    os.system(cmdline+" &")
    sys.stderr.write(cmdline+"\n")
    #sys.stdout.write("spark_master="+spark_master+"\n")
    #Here whe should have a test for master started or not.
    time.sleep(pausetime)

    # launch workers
    #cmdline = os.environ["SPARK_HOME"]+"/sbin/start-slave.sh"
    #Use spark-class because we do not want to start a service, but have a process that do not finish except in error or when killed
    # Else pbsdsh will end and stop the service
    cmdline = os.environ["SPARK_HOME"]+"/bin/spark-class org.apache.spark.deploy.worker.Worker"
    if ( memlimit is not None ):
        cmdline += " --memory "+memlimit
    if ( cpulimit is not None ):
        cmdline += " --cores "+cpulimit
    if ( workdir is not None ):
        cmdline += " --work-dir "+workdir
    cmdline += " "+spark_master
    #Overwrite command for now
    #cmdline = "$PBS_O_WORKDIR/start-slave.sh " + spark_master
    sys.stderr.write(cmdline+"\n")
    print("launching services with launcher.")
    launcher.launch(cmdline,os.environ)
    #TODO: here we should wait that the master has all its workers started
    print("sleeping " + str(pausetime))
    time.sleep(pausetime)
    print("services must be started")

# run the user's Spark "job", if one is given
if ( len(child_args)>0 ):
    print("\nLaunching spark application: " + str(child_args))
    cmdline = os.environ["SPARK_HOME"]+"/bin/spark-submit --master "+spark_master
    if ( log4j_props is not None ):
        cmdline += " --driver-java-options -Dlog4j.configuration=file:"+log4j_props
    cmdline += " "+" ".join(child_args)
    os.system(cmdline)
    print("Job end.")
else:
    #No job declared, just leave the Spark cluster started to allow connexion from any submitted job.
    #It will be stopped upon a qdel or when wall time is reached
    print("Waiting indefinitly until cluster is stopped")
    while True:
        print("Spark cluster is still alive with master" + spark_master)
        time.sleep(60)

print("\nPBS Spark Launcher End")
